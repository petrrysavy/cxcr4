import numpy as np


class UCB:
    """
    A UCB model generated by ChatGPT using the following prompt:
    <i> Give me a code for the Upper Confidence Bound model. Hold the sum and number of trials and time.
    Provide methods for picking an action and registering new reward. Use numpy arrays for storing the values.</i>
    """
    def __init__(self, n_arms, exclude_actions=[]):
        """
        Initialize the UCB model.

        Parameters:
        n_arms (int): Number of arms in the bandit problem.
        """
        self._n_arms = n_arms
        self._sums_of_rewards = np.zeros(n_arms)  # Array to hold the sum of rewards for each arm
        self._n_trials = np.zeros(n_arms, dtype=int)  # Array to hold the number of trials for each arm
        self._total_time = 0  # Tracks the total number of actions taken
        self._M2 = np.zeros(n_arms)  # sum of squares of differences from the current mean using Welford's algorithm
        self._exclude_actions = exclude_actions

    def pick_action(self, exclude_actions=None):
        """
        Pick an action (arm) based on the UCB algorithm.

        Returns:
        int: Index of the chosen arm.
        """
        if exclude_actions is None: exclude_actions = self._exclude_actions
        self._total_time += 1

        # Check for untested arms
        untested_arms = np.where(self._n_trials == 0)[0]
        if untested_arms.size > 0:
            # Return the first untested arm for exploration
            return untested_arms[0]
        untested_arms = np.setdiff1d(untested_arms, exclude_actions)

        return self.get_best_arm()

    def register_reward(self, arm, reward):
        """
        Update the UCB model with a new reward from the chosen arm.

        Parameters:
        arm (int): Index of the chosen arm.
        reward (float): Reward obtained from the chosen arm.
        """
        prev_mean = self.q_est(arm) if self._n_trials[arm] > 0 else 0
        self._sums_of_rewards[arm] += reward
        self._n_trials[arm] += 1
        self._M2[arm] += (reward - prev_mean) * (reward - self.q_est(arm))

    def n_trials(self, arm):
        return self._n_trials[arm]

    def q_est(self, arm):
        return self._sums_of_rewards[arm] / self._n_trials[arm]

    def variance(self, arm):
        if self._n_trials[arm] < 2:
            return np.nan  # not enough data
        return self._M2[arm] / (self._n_trials[arm] - 1)  # sample variance

    def stdev(self, arm):
        return np.sqrt(self.variance(arm))

    @property
    def total_time(self):
        return self._total_time

    @total_time.setter
    def total_time(self, total_time):
        '''
        This might be useful when the mdoel is part of other model and total time needs to be different.
        :param total_time: New value of total_time.
        '''
        self._total_time = total_time

    @property
    def n_arms(self):
        return self._n_arms

    def get_best_arm(self, exclude_actions=None):
        if exclude_actions is None: exclude_actions = self._exclude_actions
        average_rewards = self._sums_of_rewards / self._n_trials
        exploration_bonus = np.sqrt(2 * np.log(self._total_time) / self._n_trials)
        ucb_values = average_rewards + exploration_bonus
        for a in exclude_actions: ucb_values[a] = -np.inf

        # Return the index of the arm with the highest UCB value
        return np.argmax(ucb_values)

    def get_best_ucb_value(self, exclude_actions=None):
        if exclude_actions is None: exclude_actions = self._exclude_actions
        average_rewards = self._sums_of_rewards / self._n_trials
        exploration_bonus = np.sqrt(2 * np.log(self._total_time) / self._n_trials)
        ucb_values = average_rewards + exploration_bonus
        for a in exclude_actions: ucb_values[a] = -np.inf

        return np.nanmax(ucb_values)

    def get_greedy_q_value(self):
        return np.nanmax(self._sums_of_rewards / self._n_trials)

    def get_untested_or_best_ucb_value(self, exclude_actions=None):
        if exclude_actions is None: exclude_actions = self._exclude_actions
        untested_arms = np.where(self._n_trials == 0)[0]
        untested_arms = np.setdiff1d(untested_arms, exclude_actions)
        if untested_arms.size > 0:
            # Return maximum for untested arm for exploration
            return np.inf
        return self.get_best_ucb_value()

    def __str__(self):
        """
        Provide a string representation of the model's state.
        """
        return (f"Time: {self._total_time}, Rewards: {self._sums_of_rewards.tolist()}, "
                f"Trials: {self._n_trials.tolist()}, Best: {self.get_untested_or_best_ucb_value()}")